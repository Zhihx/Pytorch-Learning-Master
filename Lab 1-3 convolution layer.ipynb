{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5764ef49",
   "metadata": {},
   "source": [
    "\n",
    "**This ipynb file is a lecture note. And the link can be found below:**\n",
    "\n",
    "https://www.bilibili.com/video/BV1ce411K7XC?p=14&vd_source=7cca4a20f2401942703a8c8eff4d7492"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec9308",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a539e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e47e1",
   "metadata": {},
   "source": [
    "## ```torch.nn.Conv2d()```\n",
    "\n",
    "$$\\text{input image}\\xrightarrow{kernal}\\text{feature map}$$\n",
    "- $f$ kernal size\n",
    "- $n_c$ number of channels\n",
    "- $s$ stride\n",
    "- $p$ padding\n",
    "\n",
    "### descriptions\n",
    "\n",
    "```torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)```\n",
    "\n",
    "### formulas\n",
    "\n",
    "$$W_{out}=\\text{floor}\\left(\\frac{W_{in}+2p-f}{s}\\right)+1$$\n",
    "$$H_{out}=\\text{floor}\\left(\\frac{H_{in}+2p-f}{s}\\right)+1$$\n",
    "\n",
    "\n",
    "### dilated convolution\n",
    "*Dilation is a method to reduce the size of feature map. One advantage of dilation is to keep the number of learnable parameters instead of increasing it.*\n",
    "### group convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad3c60",
   "metadata": {},
   "source": [
    "### Example : Easy convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0731f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 weights torch.Size([1, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1085,  0.0888, -0.1721],\n",
      "          [ 0.1190,  0.0385, -0.1808],\n",
      "          [ 0.1075, -0.0761,  0.1911]],\n",
      "\n",
      "         [[ 0.1308, -0.0104, -0.0102],\n",
      "          [-0.1293, -0.1312, -0.1629],\n",
      "          [ 0.1627, -0.0639, -0.0252]],\n",
      "\n",
      "         [[ 0.1694,  0.1515, -0.1533],\n",
      "          [-0.1015, -0.1515, -0.1795],\n",
      "          [ 0.1819,  0.0053,  0.0895]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "c1 = nn.Conv2d(3, 1, 3)\n",
    "print('C1 weights', c1.weight.shape)\n",
    "print(c1.weight) # noted that different weights lies in different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1152f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5, 5])\n",
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones(1, 3, 5, 5)\n",
    "output = c1(input)\n",
    "print(input.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060d2b2",
   "metadata": {},
   "source": [
    "### Example : load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f075d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n",
      "tensor([[[[27., 27., 27.],\n",
      "          [27., 27., 27.],\n",
      "          [27., 27., 27.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c2 = nn.Conv2d(3, 1, 3, bias = False)\n",
    "w2 = c2.weight\n",
    "c2.weight = nn.Parameter(torch.ones_like(w2), requires_grad=True) # load pre-trained weights\n",
    "print(c2.weight)\n",
    "output = c2(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c78b17",
   "metadata": {},
   "source": [
    "## ```torch.ConvTranspose```\n",
    "\n",
    "*It had better add command to define output_size to make the output size match with input size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b00bfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of input is\n",
      " torch.Size([1, 1, 5, 5])\n",
      "The size of intermediate result is\n",
      " torch.Size([1, 2, 3, 3])\n",
      "The size of output is\n",
      " torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones(1, 1, 5, 5)\n",
    "print('The size of input is\\n', input.size())\n",
    "c3 = nn.Conv2d(1, 2, 3, stride = 2, padding = 1)\n",
    "downsample = c3(input)\n",
    "print('The size of intermediate result is\\n', downsample.shape)\n",
    "\n",
    "u4 = nn.ConvTranspose2d(2, 1, 3, stride = 2, padding = 1)\n",
    "upsample = u4(downsample, output_size = input.size())\n",
    "print('The size of output is\\n', upsample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f37c7",
   "metadata": {},
   "source": [
    "## Fold and Unfold\n",
    "\n",
    "$$\\text{Conv2D}=\\text{Unfold}(2)+\\text{Matmul}+\\text{Fold}$$\n",
    "\n",
    "In Pytorch, a convolution layer can be implemented by above three operations:\n",
    "1. Unfold kernel and input feature map to two matrix\n",
    "2. Do matrix multiplication\n",
    "3. Fold the resultant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503464b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debfb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e6801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3340b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
